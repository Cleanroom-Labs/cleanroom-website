---
title: "The Case for Privacy"
date: "2026-02-08"
author: "Lead Dev"
tags: ["security", "privacy"]
excerpt: "Privacy isn't the opposite of security — it's a component of it. The psychological, societal, and practical case for why privacy is a fundamental right, not a luxury."
slug: "the-case-for-privacy"
readTime: "16 min"
---

---

## Introduction

"Privacy is a fundamental human right." That's not a radical claim. It's Apple's [official position](https://www.npr.org/sections/alltechconsidered/2015/10/01/445026470/apple-ceo-tim-cook-privacy-is-a-fundamental-human-right), stated publicly and repeatedly by Tim Cook since 2015. It's codified in the UN Declaration of Human Rights, Article 12: "No one shall be subjected to arbitrary interference with his privacy, family, home or correspondence." It's embedded in the Fourth Amendment. It's the foundation of attorney-client privilege, doctor-patient confidentiality, and the secret ballot.

And yet, in conversations about security tools that enable privacy — encryption, air-gapping, local data processing — there's a persistent assumption that these tools exist primarily to hide wrongdoing. The framing is familiar: "If you have nothing to hide, you have nothing to fear."

This framing inverts the burden. Privacy is the default. Surveillance requires justification — not the other way around. The question isn't "why do you need privacy?" The question is "why should anyone be watching?"

This post makes the case for privacy from three angles: the documented psychological harm that surveillance causes, the societal costs when surveillance infrastructure is abused, and the positive functions that privacy serves — functions that society depends on, whether or not most people think about them consciously.

## The Psychological Cost of Surveillance

### Chilling Effects

The most measurable harm of surveillance isn't what it catches — it's what it prevents. When people know they're being watched, they change their behavior. They self-censor. They avoid topics, conversations, and even thoughts that might attract scrutiny.

This isn't speculation. In November 2013, PEN America surveyed over 520 American writers about the effects of NSA surveillance revelations on their work. The results were stark: 28% had curtailed or avoided social media activities, 24% had deliberately avoided certain topics in phone or email conversations, and 16% had avoided writing or speaking about a particular topic. One in six writers — people whose profession is the free expression of ideas — chose silence on certain subjects because they assumed someone might be listening.

The effect extends beyond writers. Legal scholar Jon Penney [studied Wikipedia traffic patterns](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2769645) before and after the Snowden disclosures in June 2013. He found a nearly 30% drop in page views for Wikipedia articles related to terrorism — topics like "al-Qaeda," "car bomb," and "dirty bomb." People weren't just avoiding saying things; they were avoiding *reading* about things. The mere awareness that metadata was being collected was enough to suppress ordinary information-seeking behavior.

The damage here isn't to any individual's privacy. It's to the information ecosystem itself. When people stop reading, writing, and talking about sensitive topics, the quality of public discourse degrades. Researchers avoid controversial subjects. Journalists lose sources. Citizens disengage from policy debates they feel they can't discuss safely. The chilling effect is a tax on the free exchange of ideas, paid invisibly by everyone.

### The Panopticon Effect

In the late eighteenth century, philosopher Jeremy Bentham designed the panopticon — a circular prison where a single guard in a central tower could observe any cell at any time. The genius of the design wasn't that every prisoner was watched constantly. It was that every prisoner *could* be watched at any moment and had no way to know when. The uncertainty itself became the mechanism of control. Prisoners would regulate their own behavior because they had to assume they might be observed.

Michel Foucault analyzed this dynamic in *Discipline and Punish* (1975), arguing that the panopticon represents something more fundamental than a prison design: it's a model for how power operates through visibility. When people internalize the possibility of surveillance, they become their own censors. The external guard becomes unnecessary because the psychological effect of potential observation does the work.

The modern digital environment is a panopticon that Bentham couldn't have imagined. Always-on smartphones, metadata collection, behavioral analytics, location tracking, and social media monitoring create an environment where the possibility of observation is constant and effectively unfalsifiable — you can never prove you aren't being watched. The psychological effect is the same one Foucault described: people adjust their behavior to account for an observer who may or may not be present, and the adjustment itself constrains freedom regardless of whether actual surveillance is occurring.

### Surveillance as Self-Fulfilling Prophecy

Perhaps the most insidious effect of surveillance is that it can create the very behaviors it claims to detect. Several well-documented psychological mechanisms contribute to this dynamic.

The Hawthorne effect — named for a [series of studies](https://www.library.hbs.edu/hc/hawthorne/09.html) at Western Electric's Hawthorne Works in the 1920s and 1930s — demonstrates that people change their behavior when they know they're being observed. In the context of surveillance, this means that monitored populations become guarded, secretive, and avoidant. These behavioral changes are then interpreted by authorities as evidence of wrongdoing, which is used to justify continued or expanded surveillance. The behavior the surveillance produces becomes the rationale for the surveillance itself.

The observer-expectancy effect compounds this. When investigators expect to find wrongdoing in a population, they interpret ambiguous behavior as confirming their suspicions. A person avoiding eye contact, using encrypted communications, or traveling by unusual routes may simply be private, anxious, or lost — but to an investigator primed to find threats, each of these becomes a data point supporting a preexisting hypothesis. The feedback loop of suspicion, scrutiny, and "confirmation" is well-documented in policing research.

Stereotype threat adds another layer. Research by Claude Steele and others has demonstrated that when people are aware of negative stereotypes about their group, they experience stress and performance changes that can paradoxically confirm those stereotypes. Applied to surveillance: communities that know they are disproportionately monitored may internalize the suspicion directed at them, experiencing anxiety, withdrawal, and behavioral changes that outside observers then point to as justification for continued monitoring.

The result is a cycle: surveillance produces alienation, alienation produces behavioral changes, behavioral changes produce "evidence" justifying more surveillance. This dynamic has been documented in studies of surveilled communities, including [research on the NYPD's surveillance of Muslim communities](https://www.aclu.org/documents/factsheet-nypd-muslim-surveillance-program) and [evaluations of the UK's Prevent program](https://doi.org/10.1057/s41599-017-0061-9), where community-level surveillance has been shown to increase alienation and distrust rather than prevent radicalization. The [Brennan Center for Justice](https://www.brennancenter.org/sites/default/files/publications/NationalSecurity_LocalPolice_web.pdf) has documented how these programs erode the community trust that effective policing depends on.

### Stress and the Cognitive Burden

The psychological toll of surveillance isn't limited to dramatic cases. A growing body of research documents the everyday stress of being monitored. A [2024 study](https://doi.org/10.1177/23294965241228874) published in *Social Currents* found that workplace surveillance — productivity monitoring, keystroke logging, screen recording — is significantly associated with reduced well-being, increased stress, and decreased job satisfaction. A [meta-analysis of 70 studies](https://doi.org/10.1016/j.chbr.2022.100227) on electronic employee monitoring found consistent negative effects on satisfaction and stress levels, with no reliable improvement in performance.

The cognitive load is real. When people are monitored, they spend mental resources managing their presentation — choosing words carefully, avoiding ambiguous actions, second-guessing their own behavior. This is cognitive effort spent not on productive work or genuine social interaction, but on performing normalcy for an audience that may or may not be watching. Over time, this constant self-management is exhausting and corrosive.

## The Societal Costs of Surveillance Abuse

The psychological effects of surveillance are not limited to individuals. When surveillance infrastructure exists, it gets misused — and the pattern is remarkably consistent across time periods and political systems.

The most thoroughly documented American example is [COINTELPRO](https://vault.fbi.gov/cointel-pro), the FBI's Counter Intelligence Program, which ran from 1956 to 1971. Under J. Edgar Hoover's direction, the FBI surveilled, infiltrated, and actively disrupted domestic political organizations including the civil rights movement, anti-war groups, and the Black Panther Party. Targets included Martin Luther King Jr., who was subjected to extensive surveillance and sent an anonymous letter encouraging him to commit suicide. The program was exposed by the [Church Committee](https://www.senate.gov/about/powers-procedures/investigations/church-committee.htm) in 1975, which concluded that intelligence agencies had "undermined the constitutional rights of citizens" through "overbroad targeting, excessive collection of information, and the use of covert action to disrupt and discredit domestic political activity."

COINTELPRO was not an aberration by a single rogue director. It was an institutional program that ran for fifteen years, involved hundreds of agents, and was directed at Americans exercising their constitutional rights. It existed because the surveillance infrastructure existed, and because the oversight mechanisms that were supposed to prevent abuse were insufficient — or absent entirely.

The most extreme historical case is East Germany's Stasi, which maintained an estimated 90,000 full-time employees and 170,000 unofficial collaborators (known as *Inoffizielle Mitarbeiter*) to monitor a population of roughly 16 million. That's approximately one informant for every 63 citizens — a penetration rate that made private conversation effectively impossible. The [social and psychological damage](https://en.wikipedia.org/wiki/Mass_surveillance_in_East_Germany) persisted for decades after reunification, manifesting as widespread distrust, damaged relationships, and a pervasive reluctance to speak freely that sociologists documented well into the 2000s.

The point of citing COINTELPRO and the Stasi is not to equate them with current surveillance practices. It is to illustrate a trajectory: when surveillance infrastructure exists without adequate accountability, it expands, it gets misused, and the consequences fall disproportionately on the people least able to challenge it. For documented contemporary examples of how U.S. surveillance infrastructure has been exploited by foreign adversaries and misused by authorized operators — from NSA analysts spying on romantic interests to the DEA hiding the origins of investigations from courts — see [Wired for Surveillance](/blog/wired-for-surveillance).

The asymmetry is structural. Surveillance infrastructure is designed by those in power and applied to everyone else. The people who authorize surveillance programs are never the ones most affected by them. Privacy is the counterbalance — the structural check that prevents the imbalance of information from becoming an imbalance of power. And surveillance capabilities, once built, are effectively never voluntarily reduced. They accumulate.

## The Positive Functions of Privacy

The case for privacy isn't only about preventing harm. Privacy serves positive functions that society depends on — functions so foundational that they're easy to take for granted until they're gone.

### Intellectual Freedom

Privacy enables thinking. Researchers, writers, and scientists need space to explore ideas — including wrong, unfashionable, or controversial ones — before those ideas are ready for public scrutiny. The "right to read" and the "right to think" depend on privacy; when reading habits and search histories are monitored, intellectual exploration becomes constrained by the awareness that someone is watching what you're curious about.

This matters practically. A researcher studying radicalization needs to read extremist material without being flagged. A journalist investigating corruption needs to search for information without tipping off the subjects of their investigation. A scientist challenging consensus needs to develop their argument without premature exposure. Academic freedom and research integrity depend on the ability to explore without observation.

### Democratic Participation

Democracy depends on spaces free from observation. The Federalist Papers were published pseudonymously — Alexander Hamilton, James Madison, and John Jay wrote under the name "Publius" precisely because the ideas needed to be evaluated on their merits, not their authors' political positions. Anonymous speech has been a cornerstone of democratic discourse since before the founding of the republic.

The secret ballot exists because open voting enables coercion. If your employer, your landlord, or your political party can see how you voted, your vote is no longer fully free. The principle extends beyond elections: labor organizing, civil rights advocacy, and political dissent all depend on the ability to associate and communicate privately. Every major social movement in American history — abolition, suffrage, civil rights, labor rights — involved private organizing by people whose views were unwelcome to those in power.

### Personal Development

People need privacy to grow, make mistakes, and change. A society without privacy is one where every past action follows you permanently — where a youthful mistake, a discarded belief, or a difficult period is preserved and accessible indefinitely.

The right to reinvention — rehabilitation after incarceration, recovery from addiction, leaving abusive relationships, changing one's mind on deeply held beliefs — requires that the past not be perfectly preserved and universally accessible. Adolescents need privacy to develop identity. People in crisis need privacy to seek help. The possibility of change depends on the possibility of a boundary between who you were and who you are becoming.

### Professional Necessity

Some of society's most important functions depend structurally on privacy.

Attorney-client privilege exists because legal representation requires confidential communication. If clients cannot speak freely to their lawyers — about what they did, what they know, what they fear — the legal system cannot function. Doctor-patient confidentiality exists because patients must be able to disclose symptoms, behaviors, and concerns fully and honestly for treatment to be effective. Journalistic source protection exists because whistleblowing and accountability journalism depend on the ability to communicate information without exposing the source to retaliation.

Trade secrets and intellectual property development depend on the ability to keep work-in-progress confidential. Companies developing proprietary technology need to protect their innovations during the vulnerable period between conception and patent or market launch. These aren't loopholes or special privileges — they're structural requirements for professions and functions that society depends on.

## Privacy in Practice

The abstract case for privacy matters, but so does the concrete one. Privacy isn't just a philosophical value — it's an operational requirement for specific, real-world activities that affect millions of people.

### Intellectual Property and Trade Secrets

Companies developing proprietary technology need to keep work product off networks that are — as [documented in detail](/blog/wired-for-surveillance) — subject to mandated surveillance infrastructure that has been compromised by foreign adversaries. This isn't paranoia. The Salt Typhoon breach targeted individuals identified as "targets of interest," and the line between political surveillance and industrial espionage is blurry at best. State-sponsored actors who can access communications metadata can identify what companies are working on, who they're partnering with, and when deals are closing.

Small companies and independent developers are particularly vulnerable. They can't afford the enterprise security infrastructure — dedicated security teams, hardware security modules, segmented networks — that large corporations deploy. For them, architectural approaches that remove network exposure entirely may be the most practical defense available.

### Sensitive Data Stewardship

Entire regulatory frameworks exist because society has decided certain categories of data must be protected:

- **Healthcare (HIPAA)**: Patient records, treatment histories, genetic information. A breach doesn't just expose data — it can affect employment, insurance, and relationships.
- **Finance (SOX, PCI-DSS)**: Transaction records, trading strategies, customer financial data. Financial privacy is the foundation of fair markets.
- **Legal (privilege)**: Case strategy, client communications, settlement negotiations. The integrity of the legal system depends on confidential attorney-client communication.
- **Government (classification)**: National security information, diplomatic communications. Classified data exists because disclosure would cause demonstrable harm.

These aren't arbitrary restrictions. They reflect a societal consensus that certain information must be protected from unauthorized access — and that the consequences of failure are severe enough to justify the cost of protection.

### Personal Privacy

Everyday communications, financial records, health information, and location data are all things that ordinary people have legitimate reasons to keep private. The right to a private life isn't conditional on having "something to hide." Everyone has information they share selectively — not because it's wrong, but because context matters. A medical condition you discuss with your doctor is not something you necessarily want your employer to know. A financial difficulty you're working through is not something you want broadcast to acquaintances. A personal relationship is not anyone else's business.

Privacy enables the trust relationships that hold society together. We share different things with different people — intimate details with close friends, professional information with colleagues, curated versions of ourselves with the public. That selectivity is not deception. It's the basic mechanism by which human relationships function. A world without it is not a more honest world — it's a less human one.

## Conclusion

Privacy isn't the opposite of security. It's a component of it.

The psychological evidence is clear: surveillance causes self-censorship, creates the behaviors it claims to detect, and imposes a cognitive burden on everyone subject to it. The historical record is consistent: surveillance infrastructure, once built, gets misused — by individuals for personal purposes, by institutions for political purposes, and by adversaries who exploit the access it provides. The positive case is equally strong: intellectual freedom, democratic participation, personal development, and critical professional functions all depend on privacy as a structural requirement, not a luxury.

Tools that enable privacy — end-to-end encryption, air-gapped computing, local data processing — aren't obstacles to legitimate law enforcement. They're protections for the vast majority of communications and data that are entirely lawful and ordinary. The alternative — building surveillance capability into infrastructure and hoping it won't be misused or compromised — has been tested for thirty years. The results are [documented](/blog/wired-for-surveillance).

Privacy is not something to be ashamed of wanting. It is something to be deliberate about protecting.

---

## References

1. Universal Declaration of Human Rights, Article 12 (1948). [un.org](https://www.un.org/en/about-us/universal-declaration-of-human-rights).

2. Cook, T., keynote speech at EPIC Champions of Freedom Award Dinner (June 2, 2015). [epic.org](https://archive.epic.org/2015/06/tim-cook-backs-privacy-crypto-.html).

3. PEN American Center, "Chilling Effects: NSA Surveillance Drives U.S. Writers to Self-Censor" (November 12, 2013). [pen.org](https://pen.org/report/chilling-effects/).

4. Penney, J. W., "Chilling Effects: Online Surveillance and Wikipedia Use," *Berkeley Technology Law Journal*, 31(1), 117–182 (2016). [ssrn.com](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2769645).

5. American Civil Liberties Union, "Factsheet: The NYPD Muslim Surveillance Program" (June 17, 2013). [aclu.org](https://www.aclu.org/documents/factsheet-nypd-muslim-surveillance-program).

6. Brennan Center for Justice, "National Security and Local Police" (2011). [brennancenter.org](https://www.brennancenter.org/sites/default/files/publications/NationalSecurity_LocalPolice_web.pdf).

7. Qurashi, F., "The Prevent strategy and the UK 'war on terror': embedding infrastructures of surveillance in Muslim communities," *Palgrave Communications*, 4, Article 17 (February 13, 2018). [nature.com](https://doi.org/10.1057/s41599-017-0061-9).

8. Glavin, P., Bierman, A., and Schieman, S., "Private Eyes, They See Your Every Move: Workplace Surveillance and Worker Well-Being," *Social Currents*, 11(4), 327–345 (2024). [doi.org](https://doi.org/10.1177/23294965241228874).

9. Siegel, R., König, C. J., and Lazar, V., "The impact of electronic monitoring on employees' job satisfaction, stress, performance, and counterproductive work behavior: A meta-analysis," *Computers in Human Behavior Reports*, 8, 100227 (2022). [sciencedirect.com](https://doi.org/10.1016/j.chbr.2022.100227).

10. U.S. Senate Select Committee to Study Governmental Operations with Respect to Intelligence Activities, *Final Report: Intelligence Activities and the Rights of Americans*, Book II, S. Rep. No. 94-755 (April 26, 1976). [senate.gov](https://www.senate.gov/about/powers-procedures/investigations/church-committee.htm).

11. FBI Records, COINTELPRO Files. [vault.fbi.gov](https://vault.fbi.gov/cointel-pro).

12. Foucault, M., *Discipline and Punish: The Birth of the Prison* (1975). Translated by Alan Sheridan, Vintage Books, 1995.

13. Koehler, J. O., *Stasi: The Untold Story of the East German Secret Police*. Boulder, CO: Westview Press (1999).

14. Landsberger, H. A., *Hawthorne Revisited*. Ithaca, NY: Cornell University (1958). See also: Harvard Business School, [The "Hawthorne Effect"](https://www.library.hbs.edu/hc/hawthorne/09.html).
